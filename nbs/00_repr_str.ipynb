{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§¾ View as a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp repr_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import test_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "\n",
    "from lovely_numpy import np_to_str_common, pretty_str, sparse_join, ansi_color, in_debugger, bytes_to_human\n",
    "from lovely_numpy import config as lnp_config\n",
    "\n",
    "from lovely_tensors.utils.config import get_config, config\n",
    "from lovely_tensors.utils.misc import to_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "torch.manual_seed(42)\n",
    "randoms = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spicy = randoms[:12].clone()\n",
    "\n",
    "spicy[0] *= 10000\n",
    "spicy[1] /= 10000\n",
    "spicy[3] = float('inf')\n",
    "spicy[4] = float('-inf')\n",
    "spicy[5] = float('nan')\n",
    "spicy = spicy.reshape((2,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "def type_to_dtype(t: str) -> torch.dtype:\n",
    "    \"Convert str, e.g. 'float32' to torch.dtype e.g torch.float32\"\n",
    "\n",
    "    dtp = vars(torch)[t]\n",
    "    assert isinstance(dtp, torch.dtype)\n",
    "    return dtp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_eq(type_to_dtype('float16'), torch.float16)\n",
    "test_eq(type_to_dtype('complex64'), torch.complex64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "dtnames = { type_to_dtype(k): v\n",
    "                for k,v in {\"float32\": \"\",\n",
    "                            \"float16\": \"f16\",\n",
    "                            \"float64\": \"f64\",\n",
    "                            \"bfloat16\": \"bf16\",\n",
    "                            \"uint8\": \"u8\", # torch does not have uint16/32/64\n",
    "                            \"int8\": \"i8\",\n",
    "                            \"int16\": \"i16\",\n",
    "                            \"int32\": \"i32\",\n",
    "                            \"int64\": \"i64\",\n",
    "                        }.items()\n",
    "}\n",
    "\n",
    "def short_dtype(x):\n",
    "    # split(\".\") to torch.uint4 -> uint4\n",
    "    return dtnames.get(x.dtype, str(x.dtype).split(\".\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_eq(short_dtype(torch.tensor(1., dtype=torch.float16)), \"f16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "def plain_repr(x: torch.Tensor):\n",
    "    \"Pick the right function to get a plain repr\"\n",
    "    return x._plain_repr() if hasattr(x, \"_plain_repr\") else repr(x)\n",
    "\n",
    "def plain_str(x: torch.Tensor):\n",
    "    \"Pick the right function to get a plain str.\"\n",
    "    return x._plain_str() if hasattr(x, \"_plain_str\") else str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "def is_nasty(t: torch.Tensor):\n",
    "    \"\"\"Return true of any `t` values are inf or nan\"\"\"\n",
    "\n",
    "    if t.numel() == 0: return False # amin/amax don't like zero-lenght tensors\n",
    "    if (t.device.type == \"meta\"): return False\n",
    "    # Unlike .min()/.max(), amin/amax do not allocate extra GPU memory.\n",
    "\n",
    "    t_min = t.amin().cpu()\n",
    "    t_max = t.amax().cpu()\n",
    "\n",
    "    return (t_min.isnan() or t_min.isinf() or t_max.isinf()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "test_eq(is_nasty(torch.tensor([1, 2, float(\"nan\")])), True)\n",
    "test_eq(is_nasty(torch.tensor([1, 2, float(\"inf\")])), True)\n",
    "test_eq(is_nasty(torch.tensor([1, 2, 3])), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "\n",
    "def torch_to_str_common(t: torch.Tensor,  # Input\n",
    "                        color=True,       # ANSI color highlighting\n",
    "                        ) -> str:\n",
    "\n",
    "    if t.numel() == 0: return ansi_color(\"empty\", \"grey\", color)\n",
    "    if t.device.type == \"meta\": return ansi_color(\"meta\", \"grey\", color)\n",
    "\n",
    "    # Unlike .min()/.max(), amin/amax do not allocate extra GPU memory.\n",
    "    amin, amax = t.amin().cpu(), t.amax().cpu()\n",
    "\n",
    "    zeros = ansi_color(\"all_zeros\", \"grey\", color) if amin.eq(0) and amax.eq(0) and t.numel() > 1 else None\n",
    "\n",
    "    summary = None\n",
    "    if not zeros:\n",
    "        minmax = f\"xâˆˆ[{pretty_str(amin)}, {pretty_str(amax)}]\" if t.numel() > 2 else None\n",
    "        meanstd = f\"Î¼={pretty_str(t.mean())} Ïƒ={pretty_str(t.std())}\" if t.numel() >= 2 else None\n",
    "        summary = sparse_join([minmax, meanstd])\n",
    "\n",
    "    return sparse_join([ summary, zeros ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "# tensor.is_cpu was only introduced in 1.13.\n",
    "def is_cpu(t: torch.Tensor):\n",
    "    return t.device == torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Fake tensors are used by torch.compile when tracing the graph, or something.\n",
    "try:\n",
    "    from torch._subclasses.fake_tensor import is_fake\n",
    "except ImportError:\n",
    "    is_fake = lambda t: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "\n",
    "@torch.no_grad()\n",
    "def to_str(t: torch.Tensor,\n",
    "            plain: bool=False,\n",
    "            verbose: bool=False,\n",
    "            depth=0,\n",
    "            lvl=0,\n",
    "            color=None) -> str:\n",
    "\n",
    "    if plain or is_fake(t):\n",
    "        return plain_repr(t)\n",
    "\n",
    "    conf = get_config()\n",
    "\n",
    "    names = getattr(t, \"names\", None)\n",
    "    if names and not all(n is None for n in names):\n",
    "        # amin/amax don't like named tensors\n",
    "        t_no_names = t.rename(None)\n",
    "    else:\n",
    "        t_no_names = t\n",
    "        names = None # If all names are None, we don't want to show them.\n",
    "\n",
    "    tname = \"tensor\" if type(t) is torch.Tensor else type(t).__name__.split(\".\")[-1]\n",
    "\n",
    "    if names:\n",
    "        shape = [f\"{n}={v}\" if n else str(v) for n,v in zip(names, t.shape)]\n",
    "        shape = f\"[{', '.join(shape)}]\"\n",
    "    else:\n",
    "        shape = str(list(t.shape)) if t.ndim else None\n",
    "    type_str = sparse_join([tname, shape], sep=\"\")\n",
    "\n",
    "    dev = str(t.device) if t.device.type != \"cpu\" else None\n",
    "    dtype = short_dtype(t)\n",
    "    grad_fn = t.grad_fn.name() if t.grad_fn else None\n",
    "    # PyTorch does not want you to know, but all `grad_fn``\n",
    "    # tensors actuall have `requires_grad=True`` too.\n",
    "    grad = \"grad\" if t.requires_grad else None\n",
    "\n",
    "\n",
    "    # For complex tensors, just show the shape / size part for now.\n",
    "    if not t.is_complex():\n",
    "        if color is None: color=conf.color\n",
    "        if in_debugger(): color=False\n",
    "        # `lovely-numpy` is used to calculate stats when doing so on GPU would require\n",
    "        # memory allocation (not float tensors, tensors with bad numbers), or if the\n",
    "        # data is on CPU (because numpy is faster).\n",
    "        #\n",
    "        # Temporarily set the numpy config to match our config for consistency.\n",
    "        with lnp_config(precision=conf.precision,\n",
    "                        threshold_min=conf.threshold_min,\n",
    "                        threshold_max=conf.threshold_max,\n",
    "                        sci_mode=conf.sci_mode):\n",
    "            if is_nasty(t_no_names) or not t.is_floating_point():\n",
    "                common = np_to_str_common(to_numpy(t), color=color, ddof=1)\n",
    "            else:\n",
    "                common = torch_to_str_common(t_no_names, color=color)\n",
    "\n",
    "            numel = None\n",
    "            nbytes = t.numel() * t.element_size()\n",
    "            if t.shape and max(t.shape) != t.numel():\n",
    "                numel = f\"n={t.numel()}\"\n",
    "                if get_config().show_mem_above <= nbytes:\n",
    "                    numel = sparse_join([numel, f\"({bytes_to_human(nbytes)})\"])\n",
    "            elif get_config().show_mem_above <= nbytes:\n",
    "                numel = bytes_to_human(nbytes)\n",
    "\n",
    "            vals = None\n",
    "            if t.device.type != \"meta\":\n",
    "                vals = pretty_str(to_numpy(t)) if 0 < t.numel() <= 10 else None\n",
    "            res = sparse_join([type_str, dtype, numel, common, grad, grad_fn, dev, vals])\n",
    "    else:\n",
    "        res = plain_repr(t)\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        res += \"\\n\" + plain_repr(t)\n",
    "\n",
    "    if depth and t.dim() > 1:\n",
    "        with config(show_mem_above=torch.inf):\n",
    "            deep_width = min((t.shape[0]), conf.deeper_width) # Print at most this many lines\n",
    "            deep_lines = [ \" \"*conf.indent*(lvl+1) + to_str(t[i,:], depth=depth-1, lvl=lvl+1, color=color)\n",
    "                                for i in range(deep_width)]\n",
    "\n",
    "            # If we were limited by width, print ...\n",
    "            if deep_width < t.shape[0]: deep_lines.append(\" \"*conf.indent*(lvl+1) + \"...\")\n",
    "\n",
    "            res += \"\\n\" + \"\\n\".join(deep_lines)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "def history_warning():\n",
    "    \"Issue a warning (once) ifw e are running in IPYthon with output cache enabled\"\n",
    "\n",
    "    if \"get_ipython\" in globals() and get_ipython().cache_size > 0:\n",
    "        warnings.warn(\"IPYthon has its output cache enabled. See https://xl0.github.io/lovely-tensors/history.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_377816/3648473780.py:6: UserWarning: IPYthon has its output cache enabled. See https://xl0.github.io/lovely-tensors/history.html\n",
      "  warnings.warn(\"IPYthon has its output cache enabled. See https://xl0.github.io/lovely-tensors/history.html\")\n"
     ]
    }
   ],
   "source": [
    "# |hide\n",
    "get_ipython().cache_size=1000\n",
    "history_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "get_ipython().cache_size=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "class StrProxy():\n",
    "    def __init__(self, t: torch.Tensor, plain=False, verbose=False, depth=0, lvl=0, color=None):\n",
    "        self.t = t\n",
    "        self.plain = plain\n",
    "        self.verbose = verbose\n",
    "        self.depth=depth\n",
    "        self.lvl=lvl\n",
    "        self.color=color\n",
    "        history_warning()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return to_str(self.t, plain=self.plain, verbose=self.verbose,\n",
    "                      depth=self.depth, lvl=self.lvl, color=self.color)\n",
    "\n",
    "    # This is used for .deeper attribute and .deeper(depth=...).\n",
    "    # The second onthe results in a __call__.\n",
    "    def __call__(self, depth=1):\n",
    "        return StrProxy(self.t, depth=depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def lovely(t: torch.Tensor, # Tensor of interest\n",
    "            verbose=False,  # Whether to show the full tensor\n",
    "            plain=False,    # Just print if exactly as before\n",
    "            depth=0,        # Show stats in depth\n",
    "            color=None):    # Force color (True/False) or auto.\n",
    "    return StrProxy(t, verbose=verbose, plain=plain, depth=depth, color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor 1.927\n",
      "tensor[2] Î¼=1.707 Ïƒ=0.311 [1.927, 1.487]\n",
      "tensor[2, 3] n=6 xâˆˆ[-2.106, 1.927] Î¼=0.276 Ïƒ=1.594 [[1.927, 1.487, 0.901], [-2.106, 0.678, -1.235]]\n",
      "tensor[11] xâˆˆ[-2.106, 1.927] Î¼=0.046 Ïƒ=1.384\n"
     ]
    }
   ],
   "source": [
    "print(lovely(randoms[0]))\n",
    "print(lovely(randoms[:2]))\n",
    "print(lovely(randoms[:6].view(2, 3))) # More than 2 elements -> show statistics\n",
    "print(lovely(randoms[:11]))           # More than 10 -> suppress data output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_eq(str(lovely(randoms[0])),             \"tensor 1.927\")\n",
    "test_eq(str(lovely(randoms[:2])),            \"tensor[2] Î¼=1.707 Ïƒ=0.311 [1.927, 1.487]\")\n",
    "test_eq(str(lovely(randoms[:6].view(2, 3))), \"tensor[2, 3] n=6 xâˆˆ[-2.106, 1.927] Î¼=0.276 Ïƒ=1.594 [[1.927, 1.487, 0.901], [-2.106, 0.678, -1.235]]\")\n",
    "test_eq(str(lovely(randoms[:11])),           \"tensor[11] xâˆˆ[-2.106, 1.927] Î¼=0.046 Ïƒ=1.384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor f64 grad 1.000\n",
      "tensor f64 grad AddBackward0 2.000\n"
     ]
    }
   ],
   "source": [
    "grad = torch.tensor(1., requires_grad=True, dtype=torch.float64)\n",
    "print(lovely(grad)); print(lovely(grad+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_eq(str(lovely(grad)), \"tensor f64 grad 1.000\")\n",
    "test_eq(str(lovely(grad+1)), \"tensor f64 grad AddBackward0 2.000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor cuda:0 1.000\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(lovely(torch.tensor(1., device=torch.device(\"cuda:0\"))))\n",
    "    test_eq(str(lovely(torch.tensor(1., device=torch.device(\"cuda:0\")))), \"tensor cuda:0 1.000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have __any__ floating point nasties? Is the tensor __all__ zeros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[2, 6] n=12 xâˆˆ[-1.605, 1.927e+04] Î¼=2.141e+03 Ïƒ=6.423e+03 \u001b[31m+Inf!\u001b[0m \u001b[31m-Inf!\u001b[0m \u001b[31mNaN!\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistics and range are calculated on good values only, if there are at lest 3 of them.\n",
    "lovely(spicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_eq(str(lovely(spicy)),\n",
    "    'tensor[2, 6] n=12 xâˆˆ[-1.605, 1.927e+04] Î¼=2.141e+03 Ïƒ=6.423e+03 \\x1b[31m+Inf!\\x1b[0m \\x1b[31m-Inf!\\x1b[0m \\x1b[31mNaN!\\x1b[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[2, 6] n=12 xâˆˆ[-1.605, 1.927e+04] Î¼=2.141e+03 Ïƒ=6.423e+03 +Inf! -Inf! NaN!"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovely(spicy, color=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[11] \u001b[31mNaN!\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovely(torch.tensor([float(\"nan\")]*11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_eq(str(lovely(torch.tensor([float(\"nan\")]*11))),\n",
    "        'tensor[11] \\x1b[31mNaN!\\x1b[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[12] \u001b[38;2;127;127;127mall_zeros\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovely(torch.zeros(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_eq(str(lovely(torch.zeros(12))),\n",
    "        'tensor[12] \\x1b[38;2;127;127;127mall_zeros\\x1b[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[0, 0, 0] f16 \u001b[38;2;127;127;127mempty\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovely(torch.randn([0,0,0], dtype=torch.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_eq(str(lovely(torch.randn([0,0,0], dtype=torch.float16))),\n",
    "        'tensor[0, 0, 0] f16 \\x1b[38;2;127;127;127mempty\\x1b[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[3] i32 xâˆˆ[1, 3] Î¼=2.000 Ïƒ=1.000 [1, 2, 3]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovely(torch.tensor([1,2,3], dtype=torch.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_eq(str(lovely(torch.tensor([1,2,3], dtype=torch.int32))),\n",
    "        'tensor[3] i32 xâˆˆ[1, 3] Î¼=2.000 Ïƒ=1.000 [1, 2, 3]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[2, 6] n=12 xâˆˆ[-1.605, 1.927e+04] Î¼=2.141e+03 Ïƒ=6.423e+03 \u001b[31m+Inf!\u001b[0m \u001b[31m-Inf!\u001b[0m \u001b[31mNaN!\u001b[0m\n",
       "tensor([[ 1.9269e+04,  1.4873e-04,  9.0072e-01,         inf,        -inf,         nan],\n",
       "        [-4.3067e-02, -1.6047e+00, -7.5214e-01,  1.6487e+00, -3.9248e-01, -1.4036e+00]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(linewidth=120)\n",
    "lovely(spicy, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269e+04,  1.4873e-04,  9.0072e-01,         inf,        -inf,         nan],\n",
       "        [-4.3067e-02, -1.6047e+00, -7.5214e-01,  1.6487e+00, -3.9248e-01, -1.4036e+00]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovely(spicy, plain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[3, 196, 196] n=115248 (0.4Mb) xâˆˆ[-2.118, 2.640] Î¼=-0.388 Ïƒ=1.073 \u001b[31mNaN!\u001b[0m\n",
       "  tensor[196, 196] n=38416 xâˆˆ[-2.118, 2.249] Î¼=-0.324 Ïƒ=1.036\n",
       "    tensor[196] xâˆˆ[-1.912, 2.249] Î¼=-0.673 Ïƒ=0.522\n",
       "    tensor[196] xâˆˆ[-1.861, 2.163] Î¼=-0.738 Ïƒ=0.418\n",
       "    tensor[196] xâˆˆ[-1.758, 2.198] Î¼=-0.806 Ïƒ=0.397\n",
       "    tensor[196] xâˆˆ[-1.656, 2.249] Î¼=-0.849 Ïƒ=0.369\n",
       "    tensor[196] xâˆˆ[-1.673, 2.198] Î¼=-0.857 Ïƒ=0.357\n",
       "    tensor[196] xâˆˆ[-1.656, 2.146] Î¼=-0.848 Ïƒ=0.372\n",
       "    tensor[196] xâˆˆ[-1.433, 2.215] Î¼=-0.784 Ïƒ=0.397\n",
       "    tensor[196] xâˆˆ[-1.279, 2.249] Î¼=-0.695 Ïƒ=0.486\n",
       "    tensor[196] xâˆˆ[-1.364, 2.249] Î¼=-0.637 Ïƒ=0.539\n",
       "    ...\n",
       "  tensor[196, 196] n=38416 xâˆˆ[-1.966, 2.429] Î¼=-0.274 Ïƒ=0.973 \u001b[31mNaN!\u001b[0m\n",
       "    tensor[196] xâˆˆ[-1.861, 2.411] Î¼=-0.529 Ïƒ=0.556\n",
       "    tensor[196] xâˆˆ[-1.826, 2.359] Î¼=-0.562 Ïƒ=0.473\n",
       "    tensor[196] xâˆˆ[-1.756, 2.376] Î¼=-0.622 Ïƒ=0.459 \u001b[31mNaN!\u001b[0m\n",
       "    tensor[196] xâˆˆ[-1.633, 2.429] Î¼=-0.664 Ïƒ=0.430\n",
       "    tensor[196] xâˆˆ[-1.651, 2.376] Î¼=-0.669 Ïƒ=0.399\n",
       "    tensor[196] xâˆˆ[-1.633, 2.376] Î¼=-0.701 Ïƒ=0.391\n",
       "    tensor[196] xâˆˆ[-1.563, 2.429] Î¼=-0.670 Ïƒ=0.380\n",
       "    tensor[196] xâˆˆ[-1.475, 2.429] Î¼=-0.616 Ïƒ=0.386\n",
       "    tensor[196] xâˆˆ[-1.511, 2.429] Î¼=-0.593 Ïƒ=0.399\n",
       "    ...\n",
       "  tensor[196, 196] n=38416 xâˆˆ[-1.804, 2.640] Î¼=-0.567 Ïƒ=1.178\n",
       "    tensor[196] xâˆˆ[-1.717, 2.396] Î¼=-0.982 Ïƒ=0.350\n",
       "    tensor[196] xâˆˆ[-1.752, 2.326] Î¼=-1.034 Ïƒ=0.314\n",
       "    tensor[196] xâˆˆ[-1.648, 2.379] Î¼=-1.086 Ïƒ=0.314\n",
       "    tensor[196] xâˆˆ[-1.630, 2.466] Î¼=-1.121 Ïƒ=0.305\n",
       "    tensor[196] xâˆˆ[-1.717, 2.448] Î¼=-1.120 Ïƒ=0.302\n",
       "    tensor[196] xâˆˆ[-1.717, 2.431] Î¼=-1.166 Ïƒ=0.314\n",
       "    tensor[196] xâˆˆ[-1.560, 2.448] Î¼=-1.124 Ïƒ=0.326\n",
       "    tensor[196] xâˆˆ[-1.421, 2.431] Î¼=-1.064 Ïƒ=0.383\n",
       "    tensor[196] xâˆˆ[-1.526, 2.396] Î¼=-1.047 Ïƒ=0.417\n",
       "    ..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.load(\"mysteryman.pt\")\n",
    "image[1,2,3] = float('nan')\n",
    "\n",
    "lovely(image, depth=2) # Limited by set_config(deeper_lines=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_377816/3561422158.py:1: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /pytorch/c10/core/TensorImpl.h:1935.)\n",
      "  t = torch.zeros(2, 3, 4, names=('N', 'C', None))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor[N=2, C=3, 4] n=24 \u001b[38;2;127;127;127mall_zeros\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.zeros(2, 3, 4, names=('N', 'C', None))\n",
    "test_eq(str(lovely(t)), \"tensor[N=2, C=3, 4] n=24 \\x1b[38;2;127;127;127mall_zeros\\x1b[0m\")\n",
    "\n",
    "lovely(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(str(lovely(torch.zeros(2,3,4), depth=1, color=False)),'tensor[2, 3, 4] n=24 all_zeros\\n  tensor[3, 4] n=12 all_zeros\\n  tensor[3, 4] n=12 all_zeros')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[3, 3] n=9 \u001b[38;2;127;127;127mmeta\u001b[0m meta"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.empty(3,3, device=\"meta\")\n",
    "lovely(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUDA memory is not leaked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before allocation: Allocated: 0 MB, Max: 0 Mb\n",
      "after allocation: Allocated: 12 MB, Max: 12 Mb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor[3, 1024, 1024] n=3145728 (12Mb) xâˆˆ[-5.013, 5.150] Î¼=-0.000 Ïƒ=0.999 cuda:0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after repr: Allocated: 12 MB, Max: 12 Mb\n",
      "after cleanup: Allocated: 0 MB, Max: 12 Mb\n"
     ]
    }
   ],
   "source": [
    "# |eval: false\n",
    "def memstats():\n",
    "    allocated = int(torch.cuda.memory_allocated() // (1024*1024))\n",
    "    max_allocated = int(torch.cuda.max_memory_allocated() // (1024*1024))\n",
    "    return f\"Allocated: {allocated} MB, Max: {max_allocated} Mb\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cudamem = torch.cuda.memory_allocated()\n",
    "    print(f\"before allocation: {memstats()}\")\n",
    "    numbers = torch.randn((3, 1024, 1024), device=\"cuda\") # 12Mb image\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    print(f\"after allocation: {memstats()}\")\n",
    "    # Note, the return value of lovely() is not a string, but a\n",
    "    # StrProxy that holds reference to 'numbers'. You have to del\n",
    "    # the references to it, but once it's gone, the reference to\n",
    "    # the tensor is gone too.\n",
    "    display(lovely(numbers) )\n",
    "    print(f\"after repr: {memstats()}\")\n",
    "\n",
    "    del numbers\n",
    "    # torch.cuda.memory.empty_cache()\n",
    "\n",
    "    print(f\"after cleanup: {memstats()}\")\n",
    "    test_eq(cudamem >= torch.cuda.memory_allocated(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4011-0.4035j,  1.1300+0.0788j, -0.0277+0.9978j, -0.4636+0.6064j, -1.1505-0.9865j])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We don't really supposed complex numbers yet\n",
    "c = torch.randn(5, dtype=torch.complex64)\n",
    "lovely(c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
