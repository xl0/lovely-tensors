{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§¾ View as a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp repr_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import test_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "\n",
    "from lovely_numpy import np_to_str_common, pretty_str, sparse_join, ansi_color\n",
    "from lovely_numpy import config as lnp_config\n",
    "\n",
    "from lovely_tensors.utils.config import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "torch.manual_seed(42)\n",
    "randoms = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasties = randoms[:12].clone()\n",
    "\n",
    "nasties[0] *= 10000\n",
    "nasties[1] /= 10000\n",
    "nasties[3] = float('inf')\n",
    "nasties[4] = float('-inf')\n",
    "nasties[5] = float('nan')\n",
    "nasties = nasties.reshape((2,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "def type_to_dtype(t: str) -> torch.dtype:\n",
    "    \"Convert str, e.g. 'float32' to torch.dtype e.g torch.float32\"\n",
    "    \n",
    "    dtp = vars(torch)[t]\n",
    "    assert isinstance(dtp, torch.dtype)\n",
    "    return dtp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_eq(type_to_dtype('float16'), torch.float16)\n",
    "test_eq(type_to_dtype('complex64'), torch.complex64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "dtnames = { type_to_dtype(k): v\n",
    "                for k,v in {\"float32\": \"\",\n",
    "                            \"float16\": \"f16\",\n",
    "                            \"float64\": \"f64\",\n",
    "                            \"uint8\": \"u8\", # torch does not have uint16/32/64\n",
    "                            \"int8\": \"i8\",\n",
    "                            \"int16\": \"i16\",\n",
    "                            \"int32\": \"i32\",\n",
    "                            \"int64\": \"i64\",\n",
    "                        }.items()\n",
    "}\n",
    "\n",
    "def short_dtype(x): return dtnames.get(x.dtype, str(x.dtype)[6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_eq(short_dtype(torch.tensor(1., dtype=torch.float16)), \"f16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "def plain_repr(x: torch.Tensor):\n",
    "    \"Pick the right function to get a plain repr\"\n",
    "    # assert isinstance(x, np.ndarray), f\"expected np.ndarray but got {type(x)}\" # Could be a sub-class.\n",
    "    return x._plain_repr() if hasattr(type(x), \"_plain_repr\") else repr(x)\n",
    "\n",
    "def plain_str(x: torch.Tensor):\n",
    "    \"Pick the right function to get a plain str.\"\n",
    "    # assert isinstance(x, np.ndarray), f\"expected np.ndarray but got {type(x)}\"\n",
    "    return x._plain_str() if hasattr(type(x), \"_plain_str\") else str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "def is_nasty(t: torch.Tensor):\n",
    "    \"\"\"Return true of any `t` values are inf or nan\"\"\"\n",
    "    \n",
    "    if t.numel() == 0: return False # amin/amax don't like zero-lenght tensors\n",
    "\n",
    "    # Unlike .min()/.max(), amin/amax do not allocate extra GPU memory.\n",
    "\n",
    "    t_min = t.amin().cpu()\n",
    "    t_max = t.amax().cpu()\n",
    "\n",
    "    return (t_min.isnan() or t_min.isinf() or t_max.isinf()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "test_eq(is_nasty(torch.tensor([1, 2, float(\"nan\")])), True)\n",
    "test_eq(is_nasty(torch.tensor([1, 2, float(\"inf\")])), True)\n",
    "test_eq(is_nasty(torch.tensor([1, 2, 3])), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "\n",
    "def torch_to_str_common(t: torch.Tensor,  # Input\n",
    "                        color=True,       # ANSI color highlighting\n",
    "                        ) -> str:\n",
    "    \n",
    "    if t.numel() == 0: return ansi_color(\"empty\", \"grey\", color)\n",
    "\n",
    "\n",
    "    # Note: At the moment the MPS backend does not support isinf or isnan.\n",
    "    # Move to CPU, as this does not cost us anything.\n",
    "    amin, amax = t.amin().cpu(), t.amax().cpu()\n",
    "\n",
    "    zeros = ansi_color(\"all_zeros\", \"grey\", color) if amin.eq(0) and amax.eq(0) and t.numel() > 1 else None\n",
    "    pinf = ansi_color(\"+Inf!\", \"red\", color) if amax.isposinf() else None\n",
    "    ninf = ansi_color(\"-Inf!\", \"red\", color) if amin.isneginf() else None\n",
    "    nan = ansi_color(\"NaN!\", \"red\", color) if amin.isnan() else None\n",
    "\n",
    "    attention = sparse_join([zeros,pinf,ninf,nan])\n",
    "    numel = f\"n={t.numel()}\" if t.numel() > 5 and max(t.shape) != t.numel() else None\n",
    "\n",
    "    summary = None\n",
    "    if not zeros:\n",
    "        minmax = f\"xâˆˆ[{pretty_str(amin)}, {pretty_str(amax)}]\" if t.numel() > 2 else None\n",
    "        meanstd = f\"Î¼={pretty_str(t.mean())} Ïƒ={pretty_str(t.std())}\" if t.numel() >= 2 else None\n",
    "        summary = sparse_join([numel, minmax, meanstd])\n",
    "\n",
    "    return sparse_join([ summary, attention])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "# tensor.is_cpu was only introduced in 1.13.\n",
    "def is_cpu(t: torch.Tensor):\n",
    "    return t.device == torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "\n",
    "@torch.no_grad()\n",
    "def to_str(t: torch.Tensor,\n",
    "            plain: bool=False,\n",
    "            verbose: bool=False,\n",
    "            depth=0,\n",
    "            lvl=0,\n",
    "            color=None) -> str:\n",
    "\n",
    "    if plain:\n",
    "        return plain_repr(t)\n",
    "\n",
    "    conf = get_config()\n",
    "\n",
    "    tname = \"tensor\" if type(t) is torch.Tensor else type(t).__name__.split(\".\")[-1]\n",
    "    shape = str(list(t.shape)) if t.ndim else None\n",
    "    type_str = sparse_join([tname, shape], sep=\"\")\n",
    "    \n",
    "    dev = str(t.device) if t.device.type != \"cpu\" else None\n",
    "    dtype = short_dtype(t)\n",
    "    grad_fn = t.grad_fn.name() if t.grad_fn else None\n",
    "    # PyTorch does not want you to know, but all `grad_fn``\n",
    "    # tensors actuall have `requires_grad=True`` too.\n",
    "    grad = \"grad\" if t.requires_grad else None \n",
    "    \n",
    "\n",
    "    # For complex tensors, just show the shape / size part for now.\n",
    "    if not t.is_complex():\n",
    "        color = conf.color if color is None else color\n",
    "        # `lovely-numpy` is used to calculate stats when doing so on GPU would require\n",
    "        # memory allocation (not float tensors, tensors with bad numbers), or if the\n",
    "        # data is on CPU (because numpy is faster).\n",
    "        #\n",
    "        # Temporarily set the numpy config to match our config for consistency.\n",
    "        with lnp_config(precision=conf.precision,\n",
    "                        threshold_min=conf.threshold_min,\n",
    "                        threshold_max=conf.threshold_max,\n",
    "                        sci_mode=conf.sci_mode):\n",
    "\n",
    "            if is_cpu(t) or is_nasty(t) or not t.is_floating_point():\n",
    "                common = np_to_str_common(t.detach().cpu().numpy(), color=color, ddof=1)\n",
    "            else:\n",
    "                common = torch_to_str_common(t, color=color)\n",
    "\n",
    "            vals = pretty_str(t.cpu().numpy()) if 0 < t.numel() <= 10 else None\n",
    "            res = sparse_join([type_str, dtype, common, grad, grad_fn, dev, vals])\n",
    "    else:\n",
    "        res = plain_repr(t)\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        res += \"\\n\" + plain_repr(t)\n",
    "\n",
    "    if depth and t.dim() > 1:\n",
    "\n",
    "        deep_width = min((t.shape[0]), conf.deeper_width) # Print at most this many lines\n",
    "        deep_lines = [ \" \"*conf.indent*(lvl+1) + to_str(t[i,:], depth=depth-1, lvl=lvl+1)\n",
    "                            for i in range(deep_width)] \n",
    "\n",
    "        # If we were limited by width, print ...\n",
    "        if deep_width < t.shape[0]: deep_lines.append(\" \"*conf.indent*(lvl+1) + \"...\")\n",
    "\n",
    "        res += \"\\n\" + \"\\n\".join(deep_lines)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "def history_warning():\n",
    "    \"Issue a warning (once) ifw e are running in IPYthon with output cache enabled\"\n",
    "\n",
    "    if \"get_ipython\" in globals() and get_ipython().cache_size > 0:\n",
    "        warnings.warn(\"IPYthon has its output cache enabled. See https://xl0.github.io/lovely-tensors/history.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_484964/3648473780.py:6: UserWarning: IPYthon has its output cache enabled. See https://xl0.github.io/lovely-tensors/history.html\n",
      "  warnings.warn(\"IPYthon has its output cache enabled. See https://xl0.github.io/lovely-tensors/history.html\")\n"
     ]
    }
   ],
   "source": [
    "# |hide\n",
    "get_ipython().cache_size=1000\n",
    "history_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "get_ipython().cache_size=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "class StrProxy():\n",
    "    def __init__(self, t: torch.Tensor, plain=False, verbose=False, depth=0, lvl=0, color=None):\n",
    "        self.t = t\n",
    "        self.plain = plain\n",
    "        self.verbose = verbose\n",
    "        self.depth=depth\n",
    "        self.lvl=lvl\n",
    "        self.color=color\n",
    "        history_warning()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return to_str(self.t, plain=self.plain, verbose=self.verbose,\n",
    "                      depth=self.depth, lvl=self.lvl, color=self.color)\n",
    "\n",
    "    # This is used for .deeper attribute and .deeper(depth=...).\n",
    "    # The second onthe results in a __call__.\n",
    "    def __call__(self, depth=1):\n",
    "        return StrProxy(self.t, depth=depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def lovely(t: torch.Tensor, # Tensor of interest\n",
    "            verbose=False,  # Whether to show the full tensor\n",
    "            plain=False,    # Just print if exactly as before\n",
    "            depth=0,        # Show stats in depth\n",
    "            color=None):    # Force color (True/False) or auto.\n",
    "    return StrProxy(t, verbose=verbose, plain=plain, depth=depth, color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor 1.927\n",
      "tensor[2] Î¼=1.707 Ïƒ=0.311 [1.927, 1.487]\n",
      "tensor[2, 3] n=6 xâˆˆ[-2.106, 1.927] Î¼=0.276 Ïƒ=1.594 [[1.927, 1.487, 0.901], [-2.106, 0.678, -1.235]]\n",
      "tensor[11] xâˆˆ[-2.106, 1.927] Î¼=0.046 Ïƒ=1.384\n"
     ]
    }
   ],
   "source": [
    "print(lovely(randoms[0]))\n",
    "print(lovely(randoms[:2]))\n",
    "print(lovely(randoms[:6].view(2, 3))) # More than 2 elements -> show statistics\n",
    "print(lovely(randoms[:11]))           # More than 10 -> suppress data output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_eq(str(lovely(randoms[0])),             \"tensor 1.927\")\n",
    "test_eq(str(lovely(randoms[:2])),            \"tensor[2] Î¼=1.707 Ïƒ=0.311 [1.927, 1.487]\")\n",
    "test_eq(str(lovely(randoms[:6].view(2, 3))), \"tensor[2, 3] n=6 xâˆˆ[-2.106, 1.927] Î¼=0.276 Ïƒ=1.594 [[1.927, 1.487, 0.901], [-2.106, 0.678, -1.235]]\")\n",
    "test_eq(str(lovely(randoms[:11])),           \"tensor[11] xâˆˆ[-2.106, 1.927] Î¼=0.046 Ïƒ=1.384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor f64 grad 1.000\n",
      "tensor f64 grad AddBackward0 2.000\n"
     ]
    }
   ],
   "source": [
    "grad = torch.tensor(1., requires_grad=True, dtype=torch.float64)\n",
    "print(lovely(grad)); print(lovely(grad+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_eq(str(lovely(grad)), \"tensor f64 grad 1.000\")\n",
    "test_eq(str(lovely(grad+1)), \"tensor f64 grad AddBackward0 2.000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor cuda:0 1.000\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(lovely(torch.tensor(1., device=torch.device(\"cuda:0\"))))\n",
    "    test_eq(str(lovely(torch.tensor(1., device=torch.device(\"cuda:0\")))), \"tensor cuda:0 1.000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have __any__ floating point nasties? Is the tensor __all__ zeros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[2, 6] n=12 xâˆˆ[-1.605, 1.927e+04] Î¼=2.141e+03 Ïƒ=6.423e+03 \u001b[31m+Inf!\u001b[0m \u001b[31m-Inf!\u001b[0m \u001b[31mNaN!\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistics and range are calculated on good values only, if there are at lest 3 of them.\n",
    "lovely(nasties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_eq(str(lovely(nasties)),\n",
    "    'tensor[2, 6] n=12 xâˆˆ[-1.605, 1.927e+04] Î¼=2.141e+03 Ïƒ=6.423e+03 \\x1b[31m+Inf!\\x1b[0m \\x1b[31m-Inf!\\x1b[0m \\x1b[31mNaN!\\x1b[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[2, 6] n=12 xâˆˆ[-1.605, 1.927e+04] Î¼=2.141e+03 Ïƒ=6.423e+03 +Inf! -Inf! NaN!"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovely(nasties, color=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[11] \u001b[31mNaN!\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovely(torch.tensor([float(\"nan\")]*11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide \n",
    "test_eq(str(lovely(torch.tensor([float(\"nan\")]*11))),\n",
    "        'tensor[11] \\x1b[31mNaN!\\x1b[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[12] \u001b[38;2;127;127;127mall_zeros\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovely(torch.zeros(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_eq(str(lovely(torch.zeros(12))),\n",
    "        'tensor[12] \\x1b[38;2;127;127;127mall_zeros\\x1b[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[0, 0, 0] f16 \u001b[38;2;127;127;127mempty\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovely(torch.randn([0,0,0], dtype=torch.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_eq(str(lovely(torch.randn([0,0,0], dtype=torch.float16))),\n",
    "        'tensor[0, 0, 0] f16 \\x1b[38;2;127;127;127mempty\\x1b[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[3] i32 xâˆˆ[1, 3] Î¼=2.000 Ïƒ=1.000 [1, 2, 3]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovely(torch.tensor([1,2,3], dtype=torch.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_eq(str(lovely(torch.tensor([1,2,3], dtype=torch.int32))),\n",
    "        'tensor[3] i32 xâˆˆ[1, 3] Î¼=2.000 Ïƒ=1.000 [1, 2, 3]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[2, 6] n=12 xâˆˆ[-1.605, 1.927e+04] Î¼=2.141e+03 Ïƒ=6.423e+03 \u001b[31m+Inf!\u001b[0m \u001b[31m-Inf!\u001b[0m \u001b[31mNaN!\u001b[0m\n",
       "tensor([[ 1.9269e+04,  1.4873e-04,  9.0072e-01,         inf,        -inf,         nan],\n",
       "        [-4.3067e-02, -1.6047e+00, -7.5214e-01,  1.6487e+00, -3.9248e-01, -1.4036e+00]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(linewidth=120)\n",
    "lovely(nasties, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269e+04,  1.4873e-04,  9.0072e-01,         inf,        -inf,         nan],\n",
       "        [-4.3067e-02, -1.6047e+00, -7.5214e-01,  1.6487e+00, -3.9248e-01, -1.4036e+00]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovely(nasties, plain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[3, 196, 196] n=115248 xâˆˆ[-2.118, 2.640] Î¼=-0.388 Ïƒ=1.073 \u001b[31mNaN!\u001b[0m\n",
       "  tensor[196, 196] n=38416 xâˆˆ[-2.118, 2.249] Î¼=-0.324 Ïƒ=1.036\n",
       "    tensor[196] xâˆˆ[-1.912, 2.249] Î¼=-0.673 Ïƒ=0.522\n",
       "    tensor[196] xâˆˆ[-1.861, 2.163] Î¼=-0.738 Ïƒ=0.418\n",
       "    tensor[196] xâˆˆ[-1.758, 2.198] Î¼=-0.806 Ïƒ=0.397\n",
       "    tensor[196] xâˆˆ[-1.656, 2.249] Î¼=-0.849 Ïƒ=0.369\n",
       "    tensor[196] xâˆˆ[-1.673, 2.198] Î¼=-0.857 Ïƒ=0.357\n",
       "    tensor[196] xâˆˆ[-1.656, 2.146] Î¼=-0.848 Ïƒ=0.372\n",
       "    tensor[196] xâˆˆ[-1.433, 2.215] Î¼=-0.784 Ïƒ=0.397\n",
       "    tensor[196] xâˆˆ[-1.279, 2.249] Î¼=-0.695 Ïƒ=0.486\n",
       "    tensor[196] xâˆˆ[-1.364, 2.249] Î¼=-0.637 Ïƒ=0.539\n",
       "    ...\n",
       "  tensor[196, 196] n=38416 xâˆˆ[-1.966, 2.429] Î¼=-0.274 Ïƒ=0.973 \u001b[31mNaN!\u001b[0m\n",
       "    tensor[196] xâˆˆ[-1.861, 2.411] Î¼=-0.529 Ïƒ=0.556\n",
       "    tensor[196] xâˆˆ[-1.826, 2.359] Î¼=-0.562 Ïƒ=0.473\n",
       "    tensor[196] xâˆˆ[-1.756, 2.376] Î¼=-0.622 Ïƒ=0.459 \u001b[31mNaN!\u001b[0m\n",
       "    tensor[196] xâˆˆ[-1.633, 2.429] Î¼=-0.664 Ïƒ=0.430\n",
       "    tensor[196] xâˆˆ[-1.651, 2.376] Î¼=-0.669 Ïƒ=0.399\n",
       "    tensor[196] xâˆˆ[-1.633, 2.376] Î¼=-0.701 Ïƒ=0.391\n",
       "    tensor[196] xâˆˆ[-1.563, 2.429] Î¼=-0.670 Ïƒ=0.380\n",
       "    tensor[196] xâˆˆ[-1.475, 2.429] Î¼=-0.616 Ïƒ=0.386\n",
       "    tensor[196] xâˆˆ[-1.511, 2.429] Î¼=-0.593 Ïƒ=0.399\n",
       "    ...\n",
       "  tensor[196, 196] n=38416 xâˆˆ[-1.804, 2.640] Î¼=-0.567 Ïƒ=1.178\n",
       "    tensor[196] xâˆˆ[-1.717, 2.396] Î¼=-0.982 Ïƒ=0.350\n",
       "    tensor[196] xâˆˆ[-1.752, 2.326] Î¼=-1.034 Ïƒ=0.314\n",
       "    tensor[196] xâˆˆ[-1.648, 2.379] Î¼=-1.086 Ïƒ=0.314\n",
       "    tensor[196] xâˆˆ[-1.630, 2.466] Î¼=-1.121 Ïƒ=0.305\n",
       "    tensor[196] xâˆˆ[-1.717, 2.448] Î¼=-1.120 Ïƒ=0.302\n",
       "    tensor[196] xâˆˆ[-1.717, 2.431] Î¼=-1.166 Ïƒ=0.314\n",
       "    tensor[196] xâˆˆ[-1.560, 2.448] Î¼=-1.124 Ïƒ=0.326\n",
       "    tensor[196] xâˆˆ[-1.421, 2.431] Î¼=-1.064 Ïƒ=0.383\n",
       "    tensor[196] xâˆˆ[-1.526, 2.396] Î¼=-1.047 Ïƒ=0.417\n",
       "    ..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.load(\"mysteryman.pt\")\n",
    "image[1,2,3] = float('nan')\n",
    "\n",
    "lovely(image, depth=2) # Limited by set_config(deeper_lines=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUDA memory is not leaked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before allocation: Allocated: 0 MB, Max: 0 Mb\n",
      "after allocation: Allocated: 12 MB, Max: 12 Mb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor[3, 1024, 1024] n=3145728 xâˆˆ[-5.325, 5.150] Î¼=-0.000 Ïƒ=0.999 cuda:0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after repr: Allocated: 12 MB, Max: 12 Mb\n",
      "after cleanup: Allocated: 0 MB, Max: 12 Mb\n"
     ]
    }
   ],
   "source": [
    "# |eval: false\n",
    "def memstats():\n",
    "    allocated = int(torch.cuda.memory_allocated() // (1024*1024))\n",
    "    max_allocated = int(torch.cuda.max_memory_allocated() // (1024*1024))\n",
    "    return f\"Allocated: {allocated} MB, Max: {max_allocated} Mb\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cudamem = torch.cuda.memory_allocated()\n",
    "    print(f\"before allocation: {memstats()}\")\n",
    "    numbers = torch.randn((3, 1024, 1024), device=\"cuda\") # 12Mb image\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    print(f\"after allocation: {memstats()}\")\n",
    "    # Note, the return value of lovely() is not a string, but a\n",
    "    # StrProxy that holds reference to 'numbers'. You have to del\n",
    "    # the references to it, but once it's gone, the reference to\n",
    "    # the tensor is gone too.\n",
    "    display(lovely(numbers) )\n",
    "    print(f\"after repr: {memstats()}\")\n",
    "    \n",
    "    del numbers\n",
    "    # torch.cuda.memory.empty_cache()\n",
    "\n",
    "    print(f\"after cleanup: {memstats()}\")\n",
    "    test_eq(cudamem >= torch.cuda.memory_allocated(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4011-0.4035j,  1.1300+0.0788j, -0.0277+0.9978j, -0.4636+0.6064j, -1.1505-0.9865j])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We don't really supposed complex numbers yet\n",
    "c = torch.randn(5, dtype=torch.complex64)\n",
    "lovely(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
